experiment:
  name: "Arxiv.Supervised_GT"
  seed: 42
  device: "cuda:0"

data_filter:
  method: "supervised"

ensemble:
  threshold: 0.99

gnn:
  # Standard GCN params
  hidden_channels: 256
  num_layers: 3
  dropout: 0.5
  lr: 0.01
  epochs: 500

dataset:
  name: "ogbn-arxiv"
  root: "./dataset"
  
  # Even if we don't use LLM, main.py might try to init the client.
  # We should keep a dummy llm config or handle it in main.py if config missing.
  # But assuming main.py expects 'llm', let's provide a minimal one or dummy so it doesn't crash.
  # However, main.py logic is: Config -> LLM Inference (maybe cached) -> Anchoring -> GNN.
  # If we want to skip LLM entirely, we should rely on cache or provide a dummy.
  
  categories: 
    - cs.NA
    # ... (usually full list here but supervised doesn't strictly need it if loader handles it)
    # But for safety, providing a short list or full list is fine. We will paste full list below.
    - cs.MM
    - cs.LO
    - cs.CY
    - cs.CR
    - cs.DC
    - cs.HC
    - cs.CE
    - cs.NI
    - cs.CC
    - cs.AI
    - cs.MA
    - cs.GL
    - cs.NE
    - cs.SC
    - cs.AR
    - cs.CV
    - cs.GR
    - cs.ET
    - cs.SY
    - cs.CG
    - cs.OH
    - cs.PL
    - cs.SE
    - cs.LG
    - cs.SD
    - cs.SI
    - cs.RO
    - cs.IT
    - cs.PF
    - cs.CL
    - cs.IR
    - cs.MS
    - cs.FL
    - cs.DS
    - cs.OS
    - cs.GT
